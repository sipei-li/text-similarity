{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseLSTM(object):\n",
    "    \"\"\"\n",
    "    A LSTM based deep Siamese network for text similarity.\n",
    "    Uses an character embedding layer, followed by a biLSTM and Energy Loss layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def BiRNN(self, x, dropout, scope, embedding_size, sequence_length, hidden_units):\n",
    "        n_hidden=hidden_units\n",
    "        n_layers=3\n",
    "        # Prepare data shape to match `static_rnn` function requirements\n",
    "        x = tf.unstack(tf.transpose(x, perm=[1, 0, 2]))\n",
    "        print(x)\n",
    "        # Define lstm cells with tensorflow\n",
    "        # Forward direction cell\n",
    "        with tf.name_scope(\"fw\"+scope),tf.variable_scope(\"fw\"+scope):\n",
    "            stacked_rnn_fw = []\n",
    "            for _ in range(n_layers):\n",
    "                fw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "                lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell,output_keep_prob=dropout)\n",
    "                stacked_rnn_fw.append(lstm_fw_cell)\n",
    "            lstm_fw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\n",
    "\n",
    "        with tf.name_scope(\"bw\"+scope),tf.variable_scope(\"bw\"+scope):\n",
    "            stacked_rnn_bw = []\n",
    "            for _ in range(n_layers):\n",
    "                bw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "                lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell,output_keep_prob=dropout)\n",
    "                stacked_rnn_bw.append(lstm_bw_cell)\n",
    "            lstm_bw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\n",
    "        # Get lstm cell output\n",
    "\n",
    "        with tf.name_scope(\"bw\"+scope),tf.variable_scope(\"bw\"+scope):\n",
    "            outputs, _, _ = tf.nn.static_bidirectional_rnn(lstm_fw_cell_m, lstm_bw_cell_m, x, dtype=tf.float32)\n",
    "        return outputs[-1]\n",
    "    \n",
    "    def contrastive_loss(self, y,d,batch_size):\n",
    "        tmp= y *tf.square(d)\n",
    "        #tmp= tf.mul(y,tf.square(d))\n",
    "        tmp2 = (1-y) *tf.square(tf.maximum((1 - d),0))\n",
    "        return tf.reduce_sum(tmp +tmp2)/batch_size/2\n",
    "    \n",
    "    def __init__(\n",
    "        self, sequence_length, vocab_size, embedding_size, hidden_units, l2_reg_lambda, batch_size):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x1 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x1\")\n",
    "        self.input_x2 = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x2\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0, name=\"l2_loss\")\n",
    "          \n",
    "        # Embedding layer\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                trainable=True,name=\"W\")\n",
    "            self.embedded_chars1 = tf.nn.embedding_lookup(self.W, self.input_x1)\n",
    "            #self.embedded_chars_expanded1 = tf.expand_dims(self.embedded_chars1, -1)\n",
    "            self.embedded_chars2 = tf.nn.embedding_lookup(self.W, self.input_x2)\n",
    "            #self.embedded_chars_expanded2 = tf.expand_dims(self.embedded_chars2, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.out1=self.BiRNN(self.embedded_chars1, self.dropout_keep_prob, \"side1\", embedding_size, sequence_length, hidden_units)\n",
    "            self.out2=self.BiRNN(self.embedded_chars2, self.dropout_keep_prob, \"side2\", embedding_size, sequence_length, hidden_units)\n",
    "            self.distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(self.out1,self.out2)),1,keep_dims=True))\n",
    "            self.distance = tf.div(self.distance, tf.add(tf.sqrt(tf.reduce_sum(tf.square(self.out1),1,keep_dims=True)),tf.sqrt(tf.reduce_sum(tf.square(self.out2),1,keep_dims=True))))\n",
    "            self.distance = tf.reshape(self.distance, [-1], name=\"distance\")\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = self.contrastive_loss(self.input_y,self.distance, batch_size)\n",
    "        #### Accuracy computation is outside of this class.\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.temp_sim = tf.subtract(tf.ones_like(self.distance),tf.rint(self.distance), name=\"temp_sim\") #auto threshold 0.5\n",
    "            correct_predictions = tf.equal(self.temp_sim, self.input_y)\n",
    "            self.accuracy=tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
